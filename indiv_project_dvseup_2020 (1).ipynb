{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Impact of NAFTA on US Wages and Income Inequality at the State Level\n",
    "#### Part I, ver 0.1\n",
    "#### Individual Term Project\n",
    "#### Prof. Flamm\n",
    "#### Spring 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preface\n",
    "This project uses a dataset created by Hakobyan and McLaren (2016)\\*, and used by them to analyze the impact of NAFTA on US wages at the national level. We are going to be using this same data in a different way.\n",
    "\n",
    "\\* [Link to Hakobyan and McLaren]('https://www.mitpressjournals.org/doi/10.1162/REST_a_00587')\n",
    "\n",
    "[Link to supplemental datafile deposited for replication when article was published]('https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/MJAPQ9')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Steps\n",
    "> First, you need to pick a state and inform me of your choice. One student per state; first come, first served.\n",
    "\n",
    "> Picking Texas because you love it (or California or New York) is a great idea but comes with a price. The dataset you will download will need to be held in memory on your laptop. Big states have lots of people and lots of observations, but use up lots of memory. If you pick a big state, you will need a laptop with lots of memory to be sure you can do the analysis in a reasonable amount of computing time. Smaller states are generally easier to deal with.\n",
    "\n",
    ">The dataframe we will be using contains observations on a U.S. census 5% sample of the U.S. population, and an estimate of the underlying population numbers that each randomly drawn observation is intended to represent. (This is called a *stratified* sample.) \n",
    "\n",
    "> Within each state, geographic areas are further subdivided into consistent (across census year) Public Use Microdata Areas (consistent PUMAs, or `conspuma`'s), the most spatially disaggregated, anonymized public use sample data on individual households that is distributed by the US Census Bureau. This data is used for many purposes by US policymakers, businesses, government agencies, nonprofits, academics, and private analysts. Any effort invested in becoming familiar with it and learning how to use it will be worthwhile.\n",
    "\n",
    ">There are 543 of these `conspuma`'s distributed across the 50 states + DC. California has the most (233), while Wyoming has the least (4). \n",
    "\n",
    "***Tip:*** If you want to get better estimates of standard errors using less stringent assumptions about unobserved statistical disturbances (*cluster robust* standard errors), you probably want a state with at least 20 `conspuma`'s to get minimally decent approximations.\n",
    "\n",
    "(May be of interest: The above rule of thumb based on published Monte Carlo studies;  new computation-intensive methods of getting standard errors through simulation that work acceptably using smaller numbers of spatial units are coming online in statistical software. But that is beyond the scope of this course.)\n",
    "\n",
    "**Conspuma Cheat Sheet**\n",
    "\n",
    "   rank  statefip\t#codes\tstname\tstusps\n",
    "\n",
    "    0\t6\t233\tCalifornia\tCA\n",
    "    1\t48\t153\tTexas\tTX\n",
    "    2\t36\t143\tNew York\tNY\n",
    "    3\t12\t127\tFlorida\tFL\n",
    "    4\t42\t92\tPennsylvania\tPA\n",
    "    5\t39\t91\tOhio\tOH\n",
    "    6\t17\t87\tIllinois\tIL\n",
    "    7\t26\t68\tMichigan\tMI\n",
    "    8\t13\t63\tGeorgia\tGA\n",
    "    9\t34\t61\tNew Jersey\tNJ\n",
    "    10\t37\t58\tNorth Carolina\tNC\n",
    "    11\t25\t52\tMassachusetts\tMA\n",
    "    12\t18\t48\tIndiana\tIN\n",
    "    13\t53\t46\tWashington\tWA\n",
    "    14\t24\t44\tMaryland\tMD\n",
    "    15\t47\t44\tTennessee\tTN\n",
    "    16\t51\t42\tVirginia\tVA\n",
    "    17\t29\t41\tMissouri\tMO\n",
    "    18\t8\t38\tColorado\tCO\n",
    "    19\t27\t37\tMinnesota\tMN\n",
    "    20\t22\t36\tLouisiana\tLA\n",
    "    21\t4\t36\tArizona\tAZ\n",
    "    22\t55\t31\tWisconsin\tWI\n",
    "    23\t1\t30\tAlabama\tAL\n",
    "    24\t21\t30\tKentucky\tKY\n",
    "    25\t41\t27\tOregon\tOR\n",
    "    26\t45\t27\tSouth Carolina\tSC\n",
    "    27\t9\t25\tConnecticut\tCT\n",
    "    28\t28\t23\tMississippi\tMS\n",
    "    29\t20\t21\tKansas\tKS\n",
    "    30\t5\t19\tArkansas\tAR\n",
    "    31\t19\t19\tIowa\tIA\n",
    "    32\t40\t18\tOklahoma\tOK\n",
    "    33\t49\t16\tUtah\tUT\n",
    "    34\t35\t15\tNew Mexico\tNM\n",
    "    35\t32\t15\tNevada\tNV\n",
    "    36\t31\t14\tNebraska\tNE\n",
    "    37\t54\t12\tWest Virginia\tWV\n",
    "    38\t33\t11\tNew Hampshire\tNH\n",
    "    39\t23\t10\tMaine\tME\n",
    "    40\t15\t9\tHawaii\tHI\n",
    "    41\t16\t9\tIdaho\tID\n",
    "    42\t44\t7\tRhode Island\tRI\n",
    "    43\t46\t7\tSouth Dakota\tSD\n",
    "    44\t30\t7\tMontana\tMT\n",
    "    45\t10\t6\tDelaware\tDE\n",
    "    46\t11\t5\tDistrict of Columbia\tDC\n",
    "    47\t2\t5\tAlaska\tAK\n",
    "    48\t38\t5\tNorth Dakota\tND\n",
    "    49\t50\t4\tVermont\tVT\n",
    "    50\t56\t4\tWyoming\tWY\n",
    "\n",
    "See `data_prep_indiv-proj_2020.ipynb` for data prep code this is taken from.\n",
    "\n",
    "[Reference for `conspuma`'s at this link.](https://usa.ipums.org/usa-action/variables/CONSPUMA#description_section)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Overview of what you will be asked to do\n",
    "> 1. In your chosen state, I will be asking you to divide the labor force into 4 groups based on educational attainment: Less than a high school degree, high school degree, some college, college graduate or even higher degree. For each group, we will be exploring what the effects of NAFTA on wages and salaries were over the 1990-2000 period. In addition, we will divide the industries in which these workers are employed into groups based on their vulnerability to Mexican imports: we can think of those industries not facing competition from Mexican imports, or with tariffs unaffected by NAFTA, as our \"control\" group, and break other industries into \"treatment\" groups depending on their level of tariff cuts.\n",
    "\n",
    "> 2. To begin, we will look at the distribution of workers across the state economy by educational group, before and after NAFTA took effect, for the the state as a whole. I will ask you do put together a chart examining how the distribution of total wage and salary income in the state between educational groups changed after NAFTA went into effect. You can use a simple bar chart, or something fancier if you are so inclined. \n",
    "\n",
    "> 3. Next, I would like you to look at at mean and median wages in your state, overall, and in each of the educational groups/industry groups, before and after NAFTA kicked in. Because this is survey data, it can get a little complicated. The variable `perwt` gives you the number of individual employees each observation represents in the underlying population. \n",
    "\n",
    "> 4. To follow up, I will ask you to do a \"skyscraper-type\" bar graph showing what percent of total state employment income went to each decile of the labor income distribution within each educational group, before and after NAFTA kicked in. I will show you in class (and a forthcoming exercise) how to calculate the share of income in a group going to a decile, making use of Python package `statsmodels`.\n",
    "\n",
    "> 5. While the previous analysis is a start at understanding what was happening, there were a lot of things going on over 1990-2000 that had nothing to do with NAFTA but may still be affecting changes in wages: wages may have been going up or down in different states and educational groups because of overall economic changes affecting all industries in a state. \n",
    "\n",
    "> Fortunately, not all locations (conspuma's) were equally affected by new competition from increased imports from Mexico stimulated by a lowering of NAFTA tariff rates. Some locations had few if any workers in industries competing with cheaper Mexican imports (because Mexico was not a competitor in that industry, or because tariffs were already very low), while other other locations were faced with large reductions in tariffs on products in which Mexico was very competitive. We can think of the latter sets of locations (opened up to Mexican competition by NAFTA) as \"treatment\" groups in an experiment on how easy it is for workers to relocate, and the former group of locations and industries (not in competition with their Mexican industry counterparts, of not facing NAFTA tariff cuts) as \"control\" groups.\n",
    "\n",
    "> One methodology we can use if we adopt this perspective is called \"difference in differences\": we compare the outcomes of two groups, both before and after a policy took effect: The treatment group: those who were affected by the policy; the control group: those who were not affected by the policy.\n",
    "\n",
    "> \"Specifically, we take the difference in outcomes of the treatment and control group before the policy was implemented, and compare it with the difference in outcomes after the policy was implemented. This method is known in economics as differences-in-differences: A method that applies an experimental research design to outcomes observed in a natural experiment. It involves comparing the difference in the average outcomes of two groups, a treatment and control group, both before and after the treatment took place. We need to compare outcomes before the policy has happened, because in a natural experiment we cannot choose exactly who receives the treatment (whereas in the lab we could randomly assign the treatment). Since the two groups are not randomly chosen, we need to account for any pre-existing differences between the two groups that could affect the outcomes, for example differences in age (for people) or characteristics (for products). If these other factors remain constant over the period considered, then we can reasonably conclude that any observed changes in the outcome differences between the groups are due to the policy. Natural experiments therefore allow us to make causal statements about policies and outcomes.\" \n",
    "[link to source of above quote](https://www.core-econ.org/doing-economics/book/text/03-01.html)\n",
    "\n",
    "> This methodology makes the ***identifying assumption*** of \"*parallel trends*\". \"It requires that in the absence of treatment, the difference between the 'treatment' and 'control' group is constant over time. Although there is no statistical test for this assumption, visual inspection is useful when you have observations over many time points.\"\n",
    "\n",
    "![Image](https://www.mailman.columbia.edu/sites/default/files/png/DIDgraph.png)\n",
    "\n",
    "> We can think of the \"parallel trends\" assumption, in the context of NAFTA, as being that wages for a given group of workers with like educational attainment and other characteristics in industries and locations affected by NAFTA tariff cuts would have moved in the same way over time as wages for industries and locations not affected by NAFTA tariff cuts, absent the NAFTA tariff cuts. \n",
    "\n",
    "> A simple linear regression model-- which allows us to control for worker characteristics, industries, and locations when we compare pre- and post- NAFTA outcomes-- is all we need to estimate a difference-in-differences model. We will talk about how to use this simple framework in part II of this project description.\n",
    "\n",
    ">6. We will also be using more advanced econometric models, which will be discussed in class, to examine NAFTA impacts as part of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
